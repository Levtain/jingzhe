# 惊蛰计划评分系统 - 设计不足分析报告

> **回顾日期**: 2025-01-06
> **回顾人**: 老黑(Claude)
> **目的**: 全面检查评分系统设计,识别潜在问题和遗漏

---

## 📊 设计完成度评估

### ✅ 已完成的核心设计

| 模块 | 完成度 | 质量 | 备注 |
|------|--------|------|------|
| **核心原则** | ✅ 100% | ⭐⭐⭐⭐⭐ | 价值观明确,革命性设计突破清晰 |
| **评分维度** | ✅ 100% | ⭐⭐⭐⭐⭐ | 6个维度定义清晰,评分范围合理 |
| **评分权限** | ✅ 100% | ⭐⭐⭐⭐⭐ | 权限矩阵完整,互斥规则明确 |
| **评分时机** | ✅ 100% | ⭐⭐⭐⭐⭐ | 修改规则合理,二次确认完善 |
| **反作弊** | ✅ 100% | ⭐⭐⭐⭐ | 基础机制完善,但可增强(见下文) |
| **AI处理** | ✅ 100% | ⭐⭐⭐⭐⭐ | 申报制+仲裁+维度不参与,逻辑严密 |
| **排名系统** | ✅ 100% | ⭐⭐⭐⭐⭐ | 革命性设计+优选游戏,创新性强 |
| **评审团权重** | ✅ 100% | ⭐⭐⭐⭐ | ×2权重合理,但需明确使用场景 |
| **排名资格** | ✅ 100% | ⭐⭐⭐⭐ | >20人次明确,但需考虑极端情况 |
| **排名计算** | ✅ 100% | ⭐⭐⭐⭐⭐ | 截尾均值+贝叶斯平滑,技术严谨 |
| **并列处理** | ✅ 100% | ⭐⭐⭐⭐⭐ | 三级tie-breaker,逻辑清晰 |

**总体完成度**: ✅ 95% - 核心设计完善,仅有少数细节需补充

---

## ⚠️ 潜在设计不足与建议

### 一、高优先级问题(建议第一届补充)

#### 🔴 问题1: 强制评分数量机制缺失

**现状**:
- 第十四章明确标记为"待补充内容"
- "是否要求参赛者至少评价N个游戏?"未确定

**风险**:
- 如果不强制,可能导致大量游戏无法达到>20人次的门槛
- 参赛者可能"搭便车",只接受评分不参与评分
- 冷启动问题加剧

**建议**:

**方案A(推荐)**: 软性强制机制
```
规则:
- 不强制最小评分数量
- 但只有评分≥N个游戏,自己的游戏才能进入排名
- 推荐: N = 10个(约占参赛作品的20-30%)

理由:
- 不会因未评分而被取消参赛资格
- 但会因不评分而失去排名机会
- 利益驱动,无需强制执行
```

**方案B**: 硬性强制
```
规则:
- 必须评分≥25个游戏
- 否则无法查看其他游戏的评分
- 无法看到自己的排名

优点: 强制力强
缺点: 体验较差,可能降低参与度
```

**建议**: 第一届采用方案A(软性强制),N=10

---

#### 🔴 问题2: 极端参赛规模应对不足

**现状**:
- 排名资格固定为">20人次"
- 未考虑参赛规模极端小(如<30个游戏)或极端大的情况

**风险场景**:

**场景A: 极小规模(20-30个游戏)**
- 如果只有20个游戏参赛,理论上每个游戏最多只能获得19个参赛者评分
- 即使有3个评审团,最多19 + 2×3 = 25人次
- 这意味着大部分游戏可能无法达到>20人次的门槛

**场景B: 极大规模(500+个游戏)**
- >20人次门槛可能太低
- 可能有大量"刚好21人次"的低质量游戏进入排名

**建议**:

**修正规则**:
```python
# 动态门槛计算
min_ratings = max(
    20,  # 绝对底线
    int(total_games * 0.3)  # 至少获得30%游戏的评分
)

# 示例:
# 20个游戏参赛 → min_ratings = max(20, 6) = 20
# 100个游戏参赛 → min_ratings = max(20, 30) = 30
# 500个游戏参赛 → min_ratings = max(20, 150) = 150
```

**第一届简化**: 如果参赛<50个游戏,可手动调整门槛为15或10

---

#### 🔴 问题3: 评审团门槛未明确

**现状**:
- 仅说明"评审团组: >5人次"
- 未说明如果评审团只有3人或5人的情况

**风险场景**:
- 如果评审团只有3人,某游戏获得3个评审团评分
- 总人次 = 0 + 2×3 = 6人次
- 远低于>20的门槛,评审团评分形同虚设

**建议**:

**修正规则**:
```python
# 参赛者组门槛
contestant_min = max(20, int(total_games * 0.3))

# 评审团组门槛(动态)
jury_min = max(
    5,  # 绝对底线
    int(total_jury_count * 0.5)  # 至少获得一半评审团的评分
)

# 示例:
# 评审团3人 → jury_min = max(5, 1) = 5
# 评审团10人 → jury_min = max(5, 5) = 5
# 评审团20人 → jury_min = max(5, 10) = 10
```

---

#### 🔴 问题4: AI维度缺失的优选游戏判定不明确

**现状**:
- 使用AI的维度不参与排名
- 但优选游戏判定"3个维度前10"如何计算?

**风险场景**:

**场景**: 某游戏使用AI生成美术
- 视觉效果: 不参与排名
- 创新性: 排名第3 ✅
- 主题诠释: 排名第5 ✅
- 音乐音频: 排名第2 ✅
- 整体性: 排名第8 ✅
- 有活儿: 排名第12

**问题**: 这个游戏是否为"优选游戏"?

**当前设计**: 不明确

**建议**:

**明确规则**:
```markdown
### 优选游戏判定规则

**计算基数**: 仅计算参与排名的维度

**判定标准**: 在参与排名的维度中,有3+个维度进入前10

**示例**:
- 游戏A: 6个维度全部参与排名,3个维度前10 → 优选游戏 ✅
- 游戏B: 使用AI美术,5个维度参与排名,3个维度前10 → 优选游戏 ✅
- 游戏C: 使用AI美术+音频,4个维度参与排名,3个维度前10 → 优选游戏 ✅
- 游戏D: 5个维度参与排名,仅2个维度前10 → 不是优选 ❌
```

**理由**: 已明确"使用AI的维度不参与",所以优选游戏判定也应该只看参与排名的维度

---

### 二、中优先级问题(第二届或边界情况考虑)

#### 🟡 问题5: 截尾均值的边界情况处理

**现状**:
- 已明确小样本(<10个)不截断
- 已明确至少截断1个
- 但未考虑极端分布情况

**边界场景**:

**场景A: 极端刷高分**
```
原始评分: [10, 10, 10, 10, 10, 10, 10, 10, 1, 1] (10个)
截断后: [1, 10, 10, 10, 10, 10, 10, 10]
截尾均值: 8.375分

问题: 如果有20个10分和2个1分会怎样?
截断后: [1, 1, 10, 10, ... 10] (去掉2个10和2个1)
截尾均值: 仍然是10分,截断无效
```

**建议**:

**第一届**: 暂不处理,依赖高危监测+人工复核

**第二届**: 考虑引入"评分分布检测"
```python
# 如果截断后所有分数都相同(方差≈0)
if variance(trimmed_scores) < 0.01:
    # 触发异常检测
    flag_for_manual_review()
```

---

#### 🟡 问题6: 贝叶斯平滑k值的合理性验证

**现状**:
- 固定k=20
- 基于IMDb的m=25000按比例推算
- 但第一届没有实际数据验证

**潜在问题**:

**场景**: 如果第一届只有50个游戏参赛
- 每个游戏平均获得20-30人次评分
- k=20可能太大,过度稀释真实评分

**建议**:

**第一届**: 保守使用k=20,但提供配置开关,允许紧急调整

**第一届结束后**: 基于实际数据重新评估
```python
# 分析第一届数据
for game in games:
    actual_count = game.rating_count
    adjustment = bayesian_adjustment_amount

# 如果大部分游戏的adjustment<0.1
# 说明k值太大,第二届可以降低k(如k=10)

# 如果大部分游戏的adjustment>0.5
# 说明k值太小,第二届可以提高k(如k=30)
```

---

#### 🟡 问题7: 并列处理的tie-breaker可能不够用

**现状**:
- 三级tie-breaker: 人次→方差→评审团占比
- 但仍可能出现完全相同的情况

**极端场景**:

**场景**: 两个游戏完全相同
```
游戏A和游戏B:
- final_score: 8.42分(完全相同)
- effective_rating_count: 45人次(完全相同)
- variance: 0.32(完全相同)
- jury_ratio: 15% (完全相同)

当前设计: 允许并列
```

**可能的问题**:
- 如果前3名都并列,第4名从哪开始?
- 如果有5个游戏并列第10,优选游戏怎么算?

**建议**:

**当前设计已合理**,但需明确:
```markdown
### 并列对优选游戏的影响

**规则**: 并列排名不影响优选游戏判定

**示例**:
- 游戏A和游戏B并列第3(创新性)
- 游戏A在其他2个维度也进入前10
- 游戏B在其他2个维度也进入前10
- 两者都是"优选游戏"
```

---

### 三、低优先级问题(未来迭代考虑)

#### 🟢 问题8: 反作弊机制的补充

**当前设计**:
- 高危监测: 10分钟内>6个游戏
- 高频差评: 6个维度均<2分

**可增强的方向**:

**第二届考虑**:
1. **时间分布分析**: 检测"最后2小时突击评分"
2. **评分模式识别**: 检测"总是给相同分数"(如全是5分或全是1分)
3. **互惠检测**: 检测"你给我高分,我给你高分"的模式

**第一届**: 暂不需要,保持简单

---

#### 🟢 问题9: 评分质量引导不足

**现状**:
- 未对"如何认真评分"提供指导
- 用户可能随意评分

**可增强的方向**:

**第一届**: 增加简单的评分提示
```
提示: 请在评价前至少游玩10分钟,给出客观公正的评分
```

**第二届**: 增加评分质量激励
```
- 评语字数>50字 → 小徽章"认真评审员"
- 评分分布合理 → 在个人页显示"评分质量认证"
```

---

#### 🟢 问题10: 数据透明度不够

**现状**:
- 用户只能看到自己的最终得分
- 无法看到"被截断了哪些评分"

**可增强的方向**:

**第一届**: 提供基础的得分解释
```
您的游戏在【创新性】维度:
- 最终得分: 8.39分
- 去掉最高10%和最低10%的极端评分
- 基于XX个有效评分计算
```

**第二届**: 提供更详细的统计信息
```
评分分布图:
[评分分布]
1分: ██ 2人
2分: █ 1人
...
10分: ███ 3人
```

---

## 📋 设计不足汇总表

| 问题 | 优先级 | 影响 | 建议方案 | 时间节点 |
|------|--------|------|---------|---------|
| **强制评分数量缺失** | 🔴 高 | 大量游戏无法达门槛 | 软性强制N=10 | 第一届必须补充 |
| **极端参赛规模应对** | 🔴 高 | <30人游戏可能全部无法排名 | 动态门槛=max(20, 30%参赛数) | 第一届必须补充 |
| **评审团门槛未明确** | 🔴 高 | 小评审团情况下评分无效 | 动态门槛=max(5, 50%评审团) | 第一届必须补充 |
| **AI维度优选判定** | 🔴 高 | 优选游戏判定逻辑不明确 | 仅看参与排名的维度 | 第一届必须补充 |
| **截尾均值边界情况** | 🟡 中 | 极端刷分可能失效 | 依赖高危监测 | 第二届评估 |
| **贝叶斯k值验证** | 🟡 中 | k=20可能不合理 | 提供配置开关,赛后调整 | 第二届验证 |
| **并列处理边界** | 🟡 中 | 多个并列时Top10计算 | 明确并列不影响优选游戏 | 第一届澄清 |
| **反作弊补充** | 🟢 低 | 可能漏过复杂刷分 | 时间分布、评分模式检测 | 第二届评估 |
| **评分质量引导** | 🟢 低 | 用户评分随意 | 增加评分提示+激励 | 第二届评估 |
| **数据透明度** | 🟢 低 | 用户看不到详细信息 | 评分分布图 | 第二届优化 |

---

## ✅ 第一届必须补充的内容清单

### 最高优先级(不开赛必须补)

**状态**: ✅ 已全部补充完成 (2025-01-06)

1. **强制评分数量机制** ✅
   - 建议采用软性强制: 必须评分≥10个游戏,自己游戏才能排名
   - 已更新: 第十四章"强制评分数量"(评分系统设计方案_v1.0.md)
   - 已更新: 2.2节"排名资格判定"(排名系统技术实现文档_v1.0.md)
   - 已更新: RankingConfig.MIN_RATED_GAMES_BY_CREATOR(ranking_algorithm_corrected.py)

2. **极端参赛规模应对** ✅
   - 动态门槛: min_ratings = max(20, 参赛作品数×30%)
   - 已更新: 第九章"排名资格要求"(评分系统设计方案_v1.0.md)
   - 已更新: 2.2节"排名资格判定"(排名系统技术实现文档_v1.0.md)
   - 已更新: check_ranking_eligibility函数(ranking_algorithm_corrected.py)

3. **评审团门槛明确** ✅
   - 动态门槛: 评审团门槛 = max(5, 评审团总数×50%)
   - 已更新: 第九章"排名资格要求"(评分系统设计方案_v1.0.md)
   - 已更新: 2.2节"排名资格判定"(排名系统技术实现文档_v1.0.md)
   - 已更新: check_ranking_eligibility函数(ranking_algorithm_corrected.py)

4. **AI维度优选判定明确** ✅
   - 仅计算参与排名的维度
   - 已更新: 7.3.1节"AI维度与优选游戏的判定"(评分系统设计方案_v1.0.md)
   - 已新增: is_preferred_game函数(ranking_algorithm_corrected.py)

**补充完成后的文档状态**:
- ✅ 评分系统设计方案_v1.0.md → v1.2(第一届完整版)
- ✅ 排名系统技术实现文档_v1.0.md → v1.1(新增动态门槛)
- ✅ ranking_algorithm_corrected.py → v1.1(新增8个修正点)

---

## 🎯 建议的优先级

### 第一届必须补充(不开赛前)
- 优先级1: 强制评分数量
- 优先级2: 极端参赛规模应对
- 优先级3: 评审团门槛明确
- 优先级4: AI维度优选判定

### 第一届期间监控(根据数据调整)
- 优先级5: 贝叶斯k值合理性
- 优先级6: 截尾均值效果

### 第二届优化
- 优先级7-10: 其他低优先级问题

---

## 📊 设计质量总体评价

### 优势 ⭐⭐⭐⭐⭐

1. **创新性强**: 革命性排名系统+AI处理机制
2. **逻辑严密**: 大部分规则都有明确说明
3. **技术严谨**: 截尾均值+贝叶斯平滑的算法经过充分论证
4. **用户友好**: 规则透明,可解释性强
5. **可扩展性**: 预留了第二届迭代的接口

### 需改进的地方

1. **边界情况**: 极端参赛规模、小评审团等情况考虑不足
2. **强制机制**: 缺少确保游戏获得足够评价的机制
3. **细节澄清**: AI维度对优选游戏的影响需明确

---

## 🎯 下一步建议

蜡烛先生,我的建议是:

**立即行动**(第一届开始前必须补充):
1. 补充强制评分数量机制(软性强制,N=10)
2. 补充极端参赛规模的应对规则
3. 补充评审团门槛的明确规则
4. 明确AI维度对优选游戏的影响

**可以等待**:
- 其他低优先级问题可以在第二届或第一届结束后根据数据决定

你觉得这个分析如何?需要我针对这4个必须补充的问题写具体的规则文本吗?
