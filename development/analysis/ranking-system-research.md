# 排名系统研究报告

> 基于itch.io和Ludum Dare的评分机制研究
>
> 研究日期: 2025-01-06
> 研究人: 老黑(Claude)
> 项目: 惊蛰计划

---

## 一、研究背景

### 1.1 用户需求

在讨论第五章"排名规则"时,用户(蜡烛先生)指出:

> "排名系统这个是我们这次比赛的重中之重啊,老黑你提出的这几个问题考虑的还是太少了。比如说,目前排名应该怎么计算才更公平?如何避免或者稀释刷分的情况这些你都没有考虑,我希望你能够针对评分系统这块儿多思考多研究一下。你能否参考一些其他比赛或者类似的有排名系统的活动,包括商业活动,给我一些参考?"

**核心关注点**:
1. 排名计算的公平性
2. 防刷分/稀释战术性评分
3. 数据可信度保障
4. 参考成熟比赛的经验

### 1.2 研究方法

用户提供了丰富的参考资料,包括:
- itch.io Game Jam评分机制讨论
- Ludum Dare官方规则
- Reddit关于GMTK Game Jam的讨论
- GitHub上的JamBrain项目issue

---

## 二、主流Game Jam排名机制对比

### 2.1 itch.io的评分调整机制

#### 核心公式
```
adjustment = √(min(median, votes_received) / median)
最终得分 = 原始平均分 × adjustment
```

#### 具体示例
假设某个jam的中位数评分次数是20次:
- 如果你的游戏获得20次或更多评分: adjustment = √(20/20) = 1.0 (无调整)
- 如果你的游戏获得19次评分: adjustment = √(19/20) = 0.97467
  - 原始4.5分 → 调整为 4.386分 (约2.5%的降低)

#### 设计逻辑
1. **基于中位数的动态调整** - 根据整个jam的参与情况自适应
2. **鼓励多参与** - 获得更多评分才能避免被"惩罚"
3. **统计学考量** - 样本越大,结果越可信,大样本应该排名更高

#### 优点 ✅
- 防止只有1-2个完美评分的小众游戏战胜数百个评分的热门游戏
- 统计学上更合理:考虑样本数量对结果可信度的影响
- 鼓励参与者积极评分他人游戏
- 自适应机制,适用于不同规模的jam

#### 缺点 ❌
- **50%的游戏必然被惩罚**(因为中位数意味着一半人低于它)
- 对小规模jam不友好(中位数可能只有10次左右,9次vs10次差异很大)
- 开发者感觉被"惩罚",用户体验不佳(itch.io管理员倾向于称为"调整"而非"惩罚")
- 容易被社交网红(有大量粉丝)主导
- 规则不透明,用户不清楚需要多少次评分才能避免"惩罚"

#### 社区反馈
根据itch.io论坛讨论:
> "Encourage and punish are two opposite things."
>
> "Personally I would prefer the Ludum Dare system - constant minimum amount of ratings, and a clear message about this."

> "Being a niche area means it is quite hard to get 'lots of votes'. More interaction can mean more votes which is a good thing, but when the difference can be in the single digits as to where the penalty line is, it's pretty disheartening for games that slide under that barrier."

---

### 2.2 Ludum Dare的硬门槛机制

#### 核心规则
```
1. 固定最低评分次数要求(例如必须>20-25次才进入排名)
2. 简单算术平均(无调整公式)
3. 不满足最低次数的游戏 → "Unranked"(不计入排名,但可显示得分)
```

#### 设计逻辑
1. **明确的准入门槛** - "必须评分>25个游戏才能进入排名"
2. **简单透明** - 规则清晰,用户容易理解
3. **要么排名,要么不排名** - 不存在"被调低"的感觉
4. **强制参与** - 鼓励参赛者积极评分他人游戏

#### 优点 ✅
- 规则简单透明:"必须评分>25个游戏才能进入排名"
- 避免了"惩罚感":要么排名,要么不排名
- 鼓励参与者积极评分他人游戏
- 公平竞争:进入排名的游戏都在同一起跑线
- 用户心理体验更好(无"被降低分数"的感觉)

#### 缺点 ❌
- 部分"被忽视的好游戏"可能因评分不足而无法排名
- 不考虑评分质量(10次评分和100次评分权重相同)
- 可能导致"刚好达标"的策略性行为

#### 实际应用
- Ludum Dare已成功使用多年
- GMTK Game Jam等大型比赛也采用类似机制
- 社区接受度高,被认为是"经过验证的可行方案"

---

### 2.3 两种机制的本质差异

| 维度 | itch.io(软调整) | Ludum Dare(硬门槛) |
|------|----------------|-------------------|
| **准入方式** | 所有游戏都能排名,但低票数被调整 | 未达门槛游戏不排名 |
| **用户感知** | "我的分数被降低了" | "我的游戏没有进入排名" |
| **激励方向** | 持续争取更多评分 | 达到最低门槛即可 |
| **适用场景** | 大型jam(1000+参与) | 中小型jam |
| **统计方法** | 样本数量影响最终得分 | 样本数量影响能否排名 |
| **复杂度** | 较高(需计算中位数和调整系数) | 低(简单计数) |
| **社区反馈** | 争议较大,小众jam体验差 | 广泛接受,体验较好 |

---

## 三、惊蛰计划当前设计回顾

### 3.1 已确定的机制

✅ **革命性排名系统**
- 12个独立排行榜(6维度 × 2评分组)
- 不计算总分,各维度独立排名
- "优选游戏"机制:在3+个维度进入Top 10

✅ **排名门槛**
- >20人次才能进入排名(类似LD的硬门槛)

✅ **反作弊机制**
- 高危监测:10分钟内评分>6个游戏 → 标记可疑
- 高频差评检测:连续差评比例异常 → 人工复核
- 评审团监督机制

### 3.2 尚未解决的问题

❓ **核心问题1: 单维度内的排名算法**
- 使用简单算术平均?
- 还是截尾平均、中位数?
- 或者贝叶斯平均?

❓ **核心问题2: 评分质量的权重**
- 是否需要根据用户"评分质量"给予不同权重?
- 高质量用户如何定义?
- 如何避免引入新的不公平?

❓ **核心问题3: 小样本问题**
- 如果某游戏刚好21人次vs 50人次,是否公平?
- 是否需要向全局平均回归?

---

## 四、三种候选方案

### 方案A: 保守方案(MVP第一届推荐)

#### 设计理念
> "简单就是美 - 采用Ludum Dare式硬门槛,第一届以稳定性为优先"

#### 具体设计

**1. 排名资格**
```
参赛者组: >20人次 AND >50%参赛作品数
评审团组: >5人次(评审团数量有限,降低门槛)

未达门槛: 显示"未达排名门槛",仍可显示平均分供参考
```

**2. 排名计算**
```
进入排名的游戏 → 简单算术平均
公式: 最终得分 = (所有评分之和) / (评分人数)

例如:
某游戏在"创新性"维度获得评分: [9,8,10,7,8,9,6,10,8,7,9,10]
最终得分 = (9+8+10+7+8+9+6+10+8+7+9+10) ÷ 12 = 8.5分
```

**3. 反作弊**
- 高危监测(已有)
- 高频差评检测(已有)
- 异常评分人工复核(已有)

#### 优势分析
| 维度 | 评分 | 理由 |
|------|------|------|
| **第一届推荐** | ⭐⭐⭐⭐⭐ | 数据不足,避免引入复杂算法风险 |
| **用户理解** | ⭐⭐⭐⭐⭐ | "平均分"概念简单直观 |
| **公平性** | ⭐⭐⭐ | 进入排名后公平,但门槛可能过滤掉好游戏 |
| **实现难度** | ⭐⭐⭐⭐⭐ | 极低,就是算术平均 |
| **技术风险** | ⭐⭐⭐⭐⭐ | 几乎无风险 |
| **数据依赖** | ⭐⭐⭐⭐⭐ | 无需历史数据 |

#### 劣势分析
- 小样本游戏(21人次)和大样本游戏(50人次)权重相同
- 极端值对结果影响较大(一个恶意差评可能显著改变排名)
- 无法体现"评分质量"的差异

#### 适用场景
- **第一届比赛**(数据不足,优先保证稳定性)
- **参赛队伍数量中等**(50-200个游戏)
- **技术团队资源有限**(避免过度设计)

---

### 方案B: 进阶方案(第二届推荐)

#### 设计理念
> "统计学优先 - 引入贝叶斯平均解决小样本问题"

#### 具体设计

**1. 排名资格**
```
同方案A
参赛者组: >20人次 AND >50%参赛作品数
评审团组: >5人次
```

**2. 排名计算(贝叶斯平均)**
```
公式: 最终得分 = (总分 + C × 全局平均分) / (评分人数 + C)

其中:
- C = 信任参数(建议设置为5-10)
- 全局平均分 = 该维度所有游戏在该评分组的平均分
```

**3. 效果示例**
```
场景1: 小样本游戏
- 原始: 21人次,平均8.5分
- 全局平均: 7.2分
- C=5

贝叶斯调整:
(8.5×21 + 7.2×5) / (21+5) = (178.5 + 36) / 26 = 8.25分
结果: 向全局平均回归0.25分

场景2: 大样本游戏
- 原始: 50人次,平均8.3分
- 全局平均: 7.2分
- C=5

贝叶斯调整:
(8.3×50 + 7.2×5) / (50+5) = (415 + 36) / 55 = 8.2分
结果: 几乎不受影响(仅降低0.1分)
```

**4. 参数C的选择**
```
C的含义: "我们有多信任全局平均分?"

C=5: 轻度调整(第二届推荐)
  - 21人次游戏向全局回归约0.25分
  - 50人次游戏几乎不受影响

C=10: 中度调整
  - 21人次游戏向全局回归约0.5分
  - 50人次游戏轻微受影响

C=20: 重度调整
  - 强制向全局平均回归
  - 不推荐(会过度稀释真实评分)
```

#### 优势分析
| 维度 | 评分 | 理由 |
|------|------|------|
| **第二届推荐** | ⭐⭐⭐⭐⭐ | 有第一届数据基础,可以更精确 |
| **用户理解** | ⭐⭐⭐ | 需要解释"为什么调整" |
| **公平性** | ⭐⭐⭐⭐ | 小样本向全局回归,更公平 |
| **实现难度** | ⭐⭐⭐⭐ | 中等,需要计算全局平均 |
| **技术风险** | ⭐⭐⭐⭐ | 低,贝叶斯平均是成熟方法 |
| **数据依赖** | ⭐⭐⭐⭐ | 需要全局平均数据 |

#### 劣势分析
- 用户理解成本较高("为什么我的8.5分变成了8.25分?")
- 需要确定合适的参数C值
- 对大样本游戏影响不大,但仍然增加计算复杂度

#### 适用场景
- **第二届比赛**(有第一届数据作为参考)
- **参赛队伍数量较多**(100+个游戏)
- **追求统计学公平性**

#### 真实案例参考
- **IMDb Top 250** 使用贝叶斯平均:
  ```
  公式: weighted rating (WR) = (v ÷ (v+m)) × R + (m ÷ (v+m)) × C
  其中:
  R = 电影平均分
  v = 评分人数
  m = 最低评分人数门槛(IMDb使用m=25000)
  C = 全局平均分
  ```

---

### 方案C: 创新方案(第三届或以后)

#### 设计理念
> "数据驱动 - 引入评分可信度权重系统,充分利用数据提高准确性"

#### 具体设计

**1. 排名资格**
```
同方案A/B
```

**2. 排名计算(加权平均)**
```python
# 伪代码示例
def calculate_weighted_score(game_ratings):
    total_weighted_score = 0
    total_weight = 0

    for rating in game_ratings:
        # 计算每个评分的权重
        weight = base_weight  # 基础权重=1.0

        # 1. 根据用户的"评分质量"调整
        if user.has_high_quality_ratings():
            weight *= 1.2  # 高质量用户加权
        elif user.is_new_rater():
            weight *= 0.8  # 新用户降权

        # 2. 根据评分的"异常程度"调整
        if rating.is_outlier():  # 与其他用户差异过大
            weight *= 0.5  # 异常评分降权

        # 3. 根据评分时机调整
        if rating.is_last_minute_rush():  # 最后2小时突击评分
            weight *= 0.7

        total_weighted_score += rating.score * weight
        total_weight += weight

    return total_weighted_score / total_weight
```

**3. "评分质量"评估标准**

✅ **高质量用户特征(权重×1.2)**:
- 评分数量多且分布合理(不是全是5星或全是1星)
- 评语详细有价值(字数>50字或被标记"有用")
- 参与时间长(非最后2小时突击评分)
- 历史评分与其他用户相关性高

⚠️ **低质量用户特征(权重×0.8)**:
- 新注册用户(评分<10个)
- 评分分布极端(全是极高分或极低分)
- 评语敷衍(字数<10字或无评语)
- 集中在最后时刻评分

❌ **异常评分特征(权重×0.5)**:
- 与其他用户对该游戏的评分差异>2分
- 与该用户对其他游戏的评分模式不一致
- 高危监测已标记的可疑评分

#### 优势分析
| 维度 | 评分 | 理由 |
|------|------|------|
| **第三届推荐** | ⭐⭐⭐⭐ | 需要两届历史数据支撑 |
| **用户理解** | ⭐⭐ | 复杂,难以解释清楚 |
| **公平性** | ⭐⭐⭐⭐⭐ | 理论上最公平 |
| **实现难度** | ⭐⭐ | 复杂,需要多维度评估 |
| **技术风险** | ⭐⭐⭐ | 中等,需要完善的算法 |
| **数据依赖** | ⭐⭐ | 需要大量历史数据 |

#### 劣势分析
- 规则复杂,用户难以理解(可能引发"权重不公"争议)
- 第一届无历史数据,难以评估"评分质量"
- 可能被"游戏化"(用户为了提高权重而行为异常)
- 实现和维护成本高

#### 适用场景
- **第三届或以后**(有充分的历史数据)
- **参赛队伍数量多**(200+个游戏)
- **追求极致的排名准确性**
- **技术团队实力强**

#### 真实案例参考
- **Reddit评论排序** 使用威尔逊得分区间,考虑:
  - 评分数量
  - 正面/负面比例
  - 统计学置信度

- **Stack Overflow reputation** 系统:
  - 根据用户贡献给予不同权重
  - 高reputation用户投票权重更高

- **GitHub贡献度评分**:
  - 考虑代码质量、频率、影响力等多维度

---

## 五、方案对比总结

### 5.1 多维度对比表

| 维度 | 方案A(保守) | 方案B(进阶) | 方案C(创新) |
|------|------------|------------|------------|
| **第一届推荐** | ✅ 强烈推荐 | ⚠️ 可选 | ❌ 不推荐 |
| **第二届推荐** | ⚠️ 可用 | ✅ 推荐 | ⚠️ 可选 |
| **第三届推荐** | ⚠️ 可用 | ⚠️ 可用 | ✅ 推荐 |
| **用户理解** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ |
| **公平性** | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| **实现难度** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐ |
| **技术风险** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ |
| **数据依赖** | 无需历史数据 | 需要全局平均 | 需要大量历史数据 |
| **维护成本** | 低 | 中 | 高 |
| **争议风险** | 极低 | 低 | 中(可能被质疑权重不公) |
| **进化路径** | → 方案B | → 方案C | - |

### 5.2 演进路线建议

```
第一届(2026): 方案A - 简单算术平均
         ↓ 收集数据,积累经验
第二届(2027): 方案B - 贝叶斯平均
         ↓ 建立用户质量评估模型
第三届(2028): 方案C - 评分可信度权重
```

---

## 六、老黑的核心建议

### 6.1 第一届强烈推荐方案A(保守方案)

#### 理由1: 规则透明
```
参赛者清楚知道"需要>20人次才能排名"
没有复杂的调整公式,没有隐藏的"惩罚"
```

#### 理由2: 实现简单
```
减少技术风险,避免第一届出bug
算术平均是所有开发者都能理解的
```

#### 理由3: 用户友好
```
无"被惩罚感",体验接近Ludum Dare
社区接受度高,争议最少
```

#### 理由4: 数据积累
```
第一届收集真实数据:
- 评分分布规律
- 刷分模式识别
- 用户行为特征
→ 为第二届优化做准备
```

### 6.2 具体实施细节建议

#### 5.1 排名资格
```markdown
- 参赛者组: >20人次 AND >50%参赛作品数
  例如: 100个游戏参与 → 需要>50人次

- 评审团组: >5人次(评审团数量有限,降低门槛)

- 未达门槛: 显示"未达排名门槛",仍可显示平均分供参考
```

#### 5.2 总分计算
```markdown
删除建议:
- 我们已确定"革命性排名系统"(各维度独立排名,无总分)
- 删除5.2章节,避免混淆
```

#### 5.3 并列处理
```markdown
- 同一维度得分相同 → 并列排名
- 例如: 第1名2个游戏(都是8.5分),第3名从下一个开始
- Top 10可能超过10个游戏(因为有并列)
```

#### 5.4 数据可信度(新增章节)
```markdown
第一届策略:
- 采用简单算术平均
- 记录所有评分数据供赛后分析
- 人工复核高危监测标记的可疑评分
- 收集数据为第二届引入贝叶斯平均做准备
```

---

## 七、待讨论的关键问题

### 7.1 排名门槛的确定

**问题**: ">20人次"是否合理?

**考虑因素**:
- 参赛队伍数量(如果只有30个游戏,50%门槛=15人次)
- 评审团规模(如果评审团只有10人,>5人次门槛可能太高)
- 强制评分规则(如果要求每个参赛者必须评分>25个游戏,则门槛可适当提高)

**建议**:
```
参赛者组门槛 = max(20, 参赛作品数 × 50%)
评审团组门槛 = max(5, 评审团总数 × 50%)

例如:
- 100个游戏参赛 → 参赛者组门槛 = max(20, 50) = 50人次
- 10个评审团 → 评审团组门槛 = max(5, 5) = 5人次
```

### 7.2 50%参赛作品数门槛的必要性

**问题**: 是否需要添加">50%参赛作品数"的附加门槛?

**支持理由**:
- 确保每个被排名的游戏都得到了广泛的评估
- 避免只有一小部分圈子内互评的游戏进入排名

**反对理由**:
- 增加了门槛复杂度
- 对于参赛作品数很多的情况(如200+个游戏),50%门槛过高

**建议**: 第一届简化处理,只使用">20人次"单一门槛

### 7.3 评审团评分的独立性

**问题**: 评审团评分是否应该完全独立于参赛者评分?

**已确定**: 评审团权重×2只影响"评分人次"统计,不影响实际分数

**待讨论**: 是否需要其他机制来体现评审团的"监督作用"?

**建议**:
```
方案1(简单): 评审团完全独立排名,无交叉影响
方案2(监督): 评审团可以标记可疑评分,供主办方复核
```

### 7.4 异常评分的处理流程

**问题**: 高危监测标记的评分如何处理?

**选项A(自动)**: 自动剔除,不参与排名计算
**选项B(半自动)**: 降低权重但不完全剔除
**选项C(人工)**: 人工复核后决定是否剔除

**建议**: 第一届采用选项C(人工复核),避免误杀

### 7.5 反作弊机制的进一步完善

**当前机制**:
- 高危监测: 10分钟内评分>6个游戏
- 高频差评检测: 连续差评比例异常

**可能的补充**:
- IP地址检测: 防止同一人多账号评分
- 设备指纹: 检测是否使用脚本批量评分
- 行为模式识别: 检测非人类的评分行为

**建议**: 第一届保持简单,优先采用现有机制

---

## 八、下一步行动

### 8.1 需要用户(蜡烛先生)确认的问题

1. **第一届是否采用方案A(保守方案)?**
2. **排名门槛是否设定为">20人次"?**
3. **是否需要添加">50%参赛作品数"的附加门槛?**
4. **评审团门槛是否设定为">5人次"?**
5. **异常评分采用人工复核还是自动剔除?**

### 8.2 需要写入设计文档的内容

1. 5.1 排名资格(确定门槛数值)
2. 5.2 总分计算(删除此章节)
3. 5.3 并列处理(明确并列规则)
4. 5.4 数据可信度(新增,说明第一届策略)

### 8.3 后续研究计划

1. **第一届比赛期间**: 收集评分数据,分析评分分布规律
2. **第一届比赛结束后**: 总结刷分模式,评估反作弊机制效果
3. **第二届规划**: 基于第一届数据,评估是否采用方案B(贝叶斯平均)

---

## 九、参考资料

### 9.1 itch.io评分机制讨论

1. **Calculated ratings for jams vs raw scores**
   - URL: https://itch.io/t/644401/calculated-ratings-for-jams-vs-raw-scores
   - 核心内容: 用户质疑"惩罚"机制的公平性,leafo管理员解释调整公式

2. **Replace the score penalty for game jam entries with too few ratings**
   - URL: https://itch.io/t/1298987/replace-the-score-penalty-for-game-jam-entries-with-too-few-ratings
   - 核心内容: 用户建议用Ludum Dare式硬门槛替代itch.io的调整机制

3. **How does the rating system of itch.io work?**
   - URL: https://itch.io/jam/coco-code-gamejam-1/topic/4569793/how-does-the-rating-system-of-itchio-work
   - 核心内容: 社区解释itch.io评分系统的工作原理

### 9.2 Ludum Dare官方规则

1. **Ludum Dare Rules**
   - URL: https://ldjam.com/events/ludum-dare/rules
   - 核心内容: 官方评分规则和排名机制

### 9.3 GitHub讨论

1. **JamBrain Project Issues**
   - 多个issue讨论Game Jam评分系统的技术实现细节
   - URL: https://github.com/JamBrain/JamBrain/issues

### 9.4 Reddit讨论

1. **GMTK Game Jam公平性讨论**
   - URL: https://www.reddit.com/r/gamedev/comments/1f0etrz/so_how_do_they_judge_the_gmtk_game_jam_fairly/
   - 核心内容: 社区讨论大型Game Jam的评分公平性

### 9.5 学术参考

1. **贝叶斯平均**
   - IMDb Top 250使用
   - 公式: WR = (v ÷ (v+m)) × R + (m ÷ (v+m)) × C

2. **威尔逊得分区间**
   - Reddit评论排序使用
   - 考虑评分数量和比例的统计学置信度

---

## 十、附录

### 10.1 术语表

- **算术平均**: 所有评分之和除以评分人数
- **贝叶斯平均**: 向先验分布(全局平均)回归的加权平均
- **中位数**: 将所有评分排序后位于中间位置的值
- **截尾平均**: 去掉最高和最低N个评分后的算术平均
- **中位数**: itch.io的调整基准点,50%的游戏低于此值
- **可信度权重**: 根据用户评分质量给予不同权重

### 10.2 关键数字速查

| 指标 | itch.io | Ludum Dare | 惊蛰计划(建议) |
|------|---------|-----------|---------------|
| **最低评分次数** | 无(中位数调整) | 20-25次(硬门槛) | 20次(硬门槛) |
| **排名计算** | 调整后平均分 | 简单算术平均 | 第一届:简单平均<br>第二届:贝叶斯平均 |
| **未达门槛** | 分数降低 | 不排名 | 不排名 |
| **评分维度** | 可自定义 | 可自定义 | 6个固定维度 |

### 10.3 决策矩阵

如果用户需要在方案A/B/C之间选择,可以参考这个决策树:

```
第一届比赛?
├─ 是 → 方案A(保守)
└─ 否 → 有第一届历史数据?
    ├─ 是 → 追求极致公平?
    │   ├─ 是 → 方案C(创新)
    │   └─ 否 → 方案B(进阶)
    └─ 否 → 方案A(保守)
```

---

**文档状态**: 初稿完成,待用户反馈后细化

**下一步**: 与蜡烛先生讨论细节,确定最终方案
