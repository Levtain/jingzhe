# 技术方案可行性回执报告

> 针对6篇排名系统技术文档对惊蛰计划的适配性研判
>
> 回执日期: 2025-01-06
> 研判人: 老黑(Claude)
> 项目: 惊蛰计划第一届比赛
> 回执对象: 蜡烛先生(产品经理)

---

## 📋 执行摘要

### 回执结论

> **6篇技术文档提供了从理论到实践的完整演进路径,但惊蛰计划第一届应采用"极简MVP策略"。**
>
> **采纳建议**: Winsorized Mean(截尾均值) + 固定k贝叶斯平滑
>
> **暂缓实施**: Huber M-估计量、动态置信度、评分者信誉权重、互评团检测

### 核心理由

1. **第一届无历史数据** - 无法支撑需要历史数据的复杂算法
2. **参赛规模50-200人** - 小样本场景,复杂算法优势不明显
3. **技术资源有限** - 创业阶段,MCMC推理、谱聚类等过于重型
4. **用户可理解性** - 简单透明规则比"黑箱算法"更容易被接受

---

## 一、惊蛰计划现状 vs 技术文档假设

### 1.1 关键差异对比表

| 维度 | 惊蛰计划第一届 | 技术文档假设 | 差异影响 |
|------|--------------|-------------|---------|
| **参赛规模** | 50-200个游戏 | 假设1000+游戏 | ❌ 不匹配 |
| **人均评分** | 20-50人次/游戏 | 假设100+人次/游戏 | ❌ 不匹配 |
| **历史数据** | 无(第一届) | 假设有历史数据学习 | ❌ 不匹配 |
| **技术团队** | 创业小团队 | 假设专业算法团队 | ❌ 不匹配 |
| **上线时间** | 需快速上线 | 假设12周迭代周期 | ❌ 不匹配 |
| **监控运维** | 基础监控 | 生产级灰度/熔断/回滚 | ⚠️ 部分匹配 |

### 1.2 现有设计基础

✅ **已确定的核心设计**(不可修改):
- 革命性排名系统: 12个独立榜单,不计算总分
- 优选游戏机制: 3个维度前10 = 优选游戏
- 排名资格: >20人次才能进入排名
- 评审团权重×2: 仅影响评分人次统计
- 反作弊机制: 高危监测(10分钟>6个游戏) + 高频差评检测

❓ **待补充的关键设计**(本次研讨重点):
- 单维度内的排名计算方法
- 并列处理规则
- 小样本问题处理
- 异常评分处理流程

---

## 二、6篇文档核心价值提取

### 2.1 文档演进脉络

```
v1.0 统计设计
  └─ 提出RRW公式、截尾均值、贝叶斯平滑
  ─→ 理论基础好,但RRW公式过于复杂

v1.0 技术评审报告
  └─ 提出层次贝叶斯模型、鲁棒统计理论
  ─→ 统计严谨,但MCMC计算开销太大

v2.0 复审意见
  └─ 提出Huber Mean + 动态贝叶斯折中方案
  ─→ 工程可行,但Huber收敛性需注意

v3.0 白皮书
  └─ 完整工程架构、RTS系统、冷启动方案
  ─→ 最接近生产,但部分算法有误

白皮书技术审查
  └─ 指出Huber误用、动态k分母为0风险
  ─→ 风险识别准确,修正建议实用

落地评审报告
  └─ MVP定义、风险清单、排期建议
  ─→ 最贴近第一届实际情况 ⭐⭐⭐⭐⭐
```

### 2.2 各文档可采纳度评级

| 文档 | 核心贡献 | 可采纳度 | 关键价值 |
|------|---------|---------|---------|
| **v1.0统计设计** | 截尾均值、贝叶斯平滑概念 | ⭐⭐⭐ | 理论基础 |
| **v1.0技术评审** | 鲁棒统计理论 | ⭐⭐ | 统计严谨性 |
| **v2.0复审意见** | Huber Mean折中方案 | ⭐⭐⭐⭐ | 工程思路 |
| **v3.0白皮书** | 完整架构设计 | ⭐⭐⭐⭐⭐ | 架构参考 |
| **技术审查报告** | 指出算法误用 | ⭐⭐⭐⭐ | 风险识别 |
| **落地评审报告** | MVP定义与排期 | ⭐⭐⭐⭐⭐ | 实操指南 |

---

## 三、关键算法可行性逐项研判

### 3.1 Huber M-估计量

#### 技术文档方案
```python
# 需要迭代求解的鲁棒均值
def huber_mean(scores, c=1.345):
    # 通过迭代重加权最小二乘法求解
    # 可能在小样本时不收敛
```

#### ❌ 第一届不推荐

**理由**:
1. **收敛性风险**: 落地评审报告指出"未给出收敛阈值,可能死循环"
2. **计算复杂度**: 需要迭代优化,第一届无必要
3. **可解释性差**: 用户会问"为什么我的8.5分变成8.3分?"

#### ✅ 推荐替代: Winsorized Mean(截尾均值)

```python
def winsorized_mean(scores, trim_percent=0.1):
    """
    截尾均值: 去掉上下各10%的极端评分后求平均

    优点:
    - 简单易懂("去掉最高和最低各10%")
    - 计算快速(O(n log n))
    - 无需迭代,必收敛
    """
    n = len(scores)
    if n < 10:  # 小样本时不截断
        return sum(scores) / n

    k = int(n * trim_percent)
    sorted_scores = sorted(scores)
    trimmed = sorted_scores[k:n-k]
    return sum(trimmed) / len(trimmed)
```

**用户可理解版本**:
> "你的得分计算方式: 去掉最高10%和最低10%的极端评分后,对剩余评分求平均。例如100个评分,去掉最高的10个和最低的10个,用中间80个计算平均分。"

---

### 3.2 动态贝叶斯平滑

#### 技术文档方案
```python
# 动态调整平滑强度
k = k0 / confidence  # confidence可能为0!
smoothed = (n * S + k * global_mean) / (n + k)
```

#### ⚠️ 第一届仅采纳固定参数版本

**理由**:
1. **除零风险**: 技术审查报告明确指出"confidence分母可能为0 → 除零panic"
2. **无历史数据**: 第一届无法准确估计confidence
3. **工程风险高**: 动态调整需要持续监控和调优

#### ✅ 推荐替代: 固定k贝叶斯平滑

```python
def fixed_bayesian_smoothing(scores, global_mean, k=20):
    """
    固定参数贝叶斯平滑

    参数说明:
    - k=20: 约需要20个评分才能摆脱全局均值的影响
    - global_mean: 该维度所有游戏的平均分

    效果:
    - 评分少 → 向全局均值回归(防止偶然高分冲榜)
    - 评分多 → 接近原始分数(大样本不被稀释)

    示例:
    - 全局平均7.2分
    - 游戏A: 3人评分,平均9.0分 → 平滑后7.8分(向7.2回归)
    - 游戏B: 50人评分,平均8.5分 → 平滑后8.4分(几乎不变)
    """
    n = len(scores)
    if n == 0:
        return global_mean

    sample_mean = sum(scores) / n
    smoothed = (n * sample_mean + k * global_mean) / (n + k)
    return smoothed
```

**用户可理解版本**:
> "为了公平,如果你的游戏评分人数较少,我们会参考全局平均水平进行调整。例如全局平均7.2分,你的游戏只有3人评分9.0分,我们会调整为7.8分,避免偶然性。随着评分人数增加,调整幅度会减小。"

**参数选择**:
- **第一届**: k=20(假设约20-30人参赛,需要大部分人都评分才能摆脱全局均值)
- **依据**: IMDb使用m=25000,我们规模小得多,按比例k=20合理

---

### 3.3 评分者信誉权重(RTS/RRW)

#### 技术文档方案
```
RTS ∈ [0,1]
影响因素: 多样性、共识偏离度、打分速度、历史行为...
聚合方式: 多因子加权公式
```

#### ❌ 第一届完全不推荐

**理由**:
1. **无历史数据**: 第一届无法评估"历史行为"
2. **公平性争议**: 用户会质疑"为什么他的权重比我高?"
3. **实施成本高**: 需要设计、开发、测试、监控、解释文档
4. **争议处理**: 一旦出现"权重不公"投诉,难以解释

#### ✅ 第一届替代: 简单规则过滤

```python
def should_include_rating(rater_behavior, rating):
    """
    第一届: 仅排除明显异常的评分
    不降权,要么全计要么不计
    """
    # 规则1: 自评排除(已有机制)
    if rater_behavior['is_self_rating']:
        return False

    # 规则2: 高危监测+高频差评 → 人工复核(已有机制)
    if (rater_behavior['is_high_risk'] and
        all_dimension_scores_low(rating)):
        return 'MANUAL_REVIEW'  # 标记为待人工复核

    # 其他情况: 全部计入,不降权
    return True
```

**第二届可考虑**: 简化版RTS(仅2个维度)
- 评分多样性(基于熵)
- 与共识的相关性
- 但需提前告知用户

---

### 3.4 互评团检测(Collusion Detection)

#### 技术文档方案
```python
# 基于图的谱聚类检测
# 时间复杂度 O(n³)
# 当n_rater>2000时,单次计算>2秒
```

#### ❌ 第一届完全不推荐

**理由**:
1. **计算复杂度**: 落地评审报告明确指出性能问题
2. **第一届规模小**: 50-200人,互评团影响有限
3. **已有机制**: 高危监测(10分钟>6个游戏)已覆盖大部分情况

#### ✅ 第一届替代: 事后离线分析

```python
def post_event_analysis(all_ratings):
    """
    第一届结束后,离线分析互评团情况
    不影响实时排名,仅用于第二届改进
    """
    # 1. 简单相似度计算(不用谱聚类)
    suspicious_pairs = []
    for i, j in all_rater_pairs:
        correlation = pearson_correlation(
            ratings[i],
            ratings[j]
        )
        if correlation > 0.8:  # 相关系数>0.8
            suspicious_pairs.append((i, j))

    # 2. 人工审查可疑对子
    return suspicious_pairs
```

**第二届考虑**: 如果第一届确实发现互评团问题,再考虑实时检测

---

### 3.5 不确定性惩罚(μ − λσ)

#### 技术文档方案
```python
final_score = smoothed_score - lambda * std_dev
# lambda=0.3, std_dev=评分标准差
```

#### ❌ 第一届不推荐

**理由**:
1. **技术审查报告指出**: "高方差不一定代表低质量,可能代表创新性作品的两极分化"
2. **λ值任意**: 落地评审报告建议"先用0.3,但需埋点记录敏感度",第一届无A/B能力
3. **用户体验差**: 用户会问"为什么我的分数被减了?"

#### ✅ 替代方案: Tie-breaker规则

```python
def rank_games(games):
    """
    排名时,同分情况下的tie-breaker:
    1. 评分人数多的优先(样本更大,更可信)
    2. 方差小的优先(更稳定)
    """
    games.sort(key=lambda g: g['score'], reverse=True)

    # 处理同分
    i = 0
    while i < len(games) - 1:
        if abs(games[i]['score'] - games[i+1]['score']) < 0.001:
            # 同分,应用tie-breaker
            if games[i]['rating_count'] != games[i+1]['rating_count']:
                # 人数多的优先
                if games[i]['rating_count'] < games[i+1]['rating_count']:
                    games[i], games[i+1] = games[i+1], games[i]
            else:
                # 人数相同,方差小的优先
                if games[i]['variance'] > games[i+1]['variance']:
                    games[i], games[i+1] = games[i+1], games[i]
        i += 1

    return games
```

**符合"革命性排名系统"**:
- 不调整分数本身
- 仅影响排名顺序
- 更容易被用户接受

---

## 四、第一届MVP最终推荐方案

### 4.1 完整排名计算流程

```python
def calculate_game_ranking(game, dimension, global_stats):
    """
    惊蛰计划第一届 - 单维度排名计算

    输入:
    - game: 游戏对象
    - dimension: 评分维度(如"创新性")
    - global_stats: 全局统计信息

    输出:
    - ranking_info: 排名信息字典
    """

    # ===== 第1步: 收集评分 =====
    all_ratings = game.get_ratings(dimension)
    contestant_ratings = [r for r in all_ratings if r.rater_type == 'contestant']
    jury_ratings = [r for r in all_ratings if r.rater_type == 'jury']

    # ===== 第2步: 过滤异常评分 =====
    valid_contestant = filter_anomalous(contestant_ratings)
    valid_jury = filter_anomalous(jury_ratings)

    # ===== 第3步: 计算评分人次 =====
    rating_count = len(valid_contestant) + 2 * len(valid_jury)

    # ===== 第4步: 检查排名资格 =====
    MIN_RATINGS = 20  # 已确定规则
    if rating_count < MIN_RATINGS:
        return {
            'is_ranked': False,
            'raw_mean': calculate_mean(valid_contestant + valid_jury),
            'rating_count': rating_count,
            'reason': '未达排名门槛(需>20人次)'
        }

    # ===== 第5步: 合并评分(不分参赛者/评审团) =====
    all_valid_ratings = valid_contestant + valid_jury
    raw_scores = [r.score for r in all_valid_ratings]

    # ===== 第6步: 鲁棒均值(Winsorized Mean) =====
    robust_score = winsorized_mean(raw_scores, trim_percent=0.1)

    # ===== 第7步: 贝叶斯平滑(固定k=20) =====
    global_mean = global_stats[dimension]['mean']
    final_score = fixed_bayesian_smoothing(
        raw_scores,
        global_mean,
        k=20
    )

    # ===== 第8步: 输出排名信息 =====
    return {
        'is_ranked': True,
        'final_score': final_score,
        'raw_mean': calculate_mean(raw_scores),
        'robust_mean': robust_score,
        'rating_count': rating_count,
        'contestant_count': len(valid_contestant),
        'jury_count': len(valid_jury),
        'variance': calculate_variance(raw_scores)
    }
```

### 4.2 配置参数建议

| 参数 | 第一届建议值 | 理由 | 未来调整 |
|------|------------|------|---------|
| **min_ratings** | 20人次 | 已确定规则 | 无需调整 |
| **trim_percent** | 10% | 去掉上下各10%极端值 | 可调至5-15% |
| **k(贝叶斯)** | 20 | 约需20个评分摆脱全局均值 | 根据数据调整 |
| **lambda** | 不使用 | 避免争议 | 第二届评估 |

### 4.3 用户说明示例(FAQ)

**Q1: 我的得分是怎么计算的?**

> A: 我们采用两步计算:
> 1. **去掉极端值**: 去掉最高10%和最低10%的评分,防止恶意刷分
> 2. **贝叶斯平滑**: 如果你的游戏评分人数较少,会参考全局平均水平调整
>
> 示例: 全局平均7.2分,你的游戏只有3人评9.0分,我们会调整为7.8分,避免偶然性。如果有50人评8.5分,调整后仍为8.5分左右。

**Q2: 为什么我的游戏没有进入排名?**

> A: 需要获得>20人次的评分才能进入最终排名。如果未达到门槛,仍会显示平均分供参考,但不计入正式排名。

**Q3: 如果有人恶意给我差评怎么办?**

> A: 我们有两重保护:
> 1. **截尾均值**: 自动去掉最低10%的极端评分
> 2. **高危监测**: 如果某人在10分钟内给6个以上游戏打低分,会被标记并人工复核

**Q4: 评审团的评分权重更高吗?**

> A: 评审团的每个评分在计算"评分人次"时按2人次计算,但实际分数上与参赛者评分权重相同。例如15个参赛者评分+3个评审团评分=21人次,达到>20人次的排名门槛。

---

## 五、与现有设计的整合方案

### 5.1 无需修改的部分 ✅

- **革命性排名系统**: 12个独立榜单,不计算总分
- **优选游戏机制**: 3个维度前10 = 优选游戏
- **排名资格**: >20人次才能进入排名
- **评审团权重×2**: 仅影响评分人次统计
- **反作弊机制**: 高危监测+高频差评检测
- **AI维度处理**: 使用AI的维度不参与排名

### 5.2 需要补充的内容 ⚠️

**需要在[评分系统设计方案_v1.0.md](../design/评分系统设计方案_v1.0.md)中新增**:

**第5.2节 排名计算方法** (新增)
```markdown
### 排名计算方法(第一届)

某游戏在某维度的最终得分计算步骤:

#### 1. 过滤异常评分
- 排除自评(已有机制)
- 标记高危监测+高频差评的评分,人工复核

#### 2. 检查排名资格
- 总评分人次 > 20 → 进入排名
- 总评分人次 ≤ 20 → 不排名,显示"未达排名门槛"

#### 3. 计算鲁棒均值(抗极端值)
- 采用Winsorized Mean(截尾均值)
- 去掉上下各10%的极端评分
- 对剩余评分求算术平均
- 小样本(<10个评分)不截断,直接算术平均

#### 4. 贝叶斯平滑(解决冷启动)
- 固定参数k=20
- 公式: S' = (n × S + 20 × μ) / (n + 20)
- 效果: 小样本向全局均值回归,大样本几乎不受影响

#### 5. 输出最终得分
- 进入排名的游戏: 显示最终得分
- 未达门槛的游戏: 显示原始平均分(供参考)
```

**第5.3节 并列处理规则** (新增)
```markdown
### 并列处理规则

同一维度得分相同时,tie-breaker优先级:

1. **评分人数多的优先**
   - 理由: 样本越大,结果越可信
   - 示例: 都是8.5分,A有30人评分,B有25人评分 → A排名更高

2. **方差小的优先**(如果人数也相同)
   - 理由: 更稳定的评分更可靠
   - 示例: 都是8.5分,都是30人评分,A方差0.5,B方差0.8 → A排名更高

3. **完全相同** → 并列排名
   - 示例: 第1名2个游戏,第3名从下一个开始
   - Top 10可能超过10个游戏
```

---

## 六、实施计划与风险控制

### 6.1 开发排期

| 阶段 | 任务 | 工作量 | 负责人 | 交付物 |
|------|------|--------|--------|--------|
| **第1天** | 确认技术方案 | 0.5天 | 产品+技术 | 方案确认文档 |
| | 设计FAQ说明 | 0.5天 | 运营 | 用户可理解版本 |
| **第2-3天** | 实现排名算法 | 2天 | 后端开发 | 代码+单元测试 |
| **第4天** | 集成测试 | 1天 | QA | 测试报告 |
| **第5天** | 文档更新 | 0.5天 | 产品 | 更新设计文档 |
| | 性能测试 | 0.5天 | QA | 压测报告 |

**总计**: 5人日,可在1周内完成

### 6.2 技术风险与应对

| 风险项 | 风险等级 | 应对措施 | 责任人 |
|--------|---------|---------|--------|
| **Winsorized Mean小样本** | 低 | n<10时不截断,直接算术平均 | 后端开发 |
| **贝叶斯k值不当** | 中 | 提供配置开关,可快速调整 | 后端开发 |
| **极端值处理争议** | 中 | 人工复核+申诉机制 | 运营 |
| **计算性能** | 极低 | 简单算法,无性能问题 | - |

### 6.3 监控指标(第一届比赛期间)

**需要监控的数据**:
1. 评分分布(是否符合正态分布?)
2. 极端值比例(是否需要调整截断比例?)
3. 并列情况(是否tie-breaker发挥作用?)
4. 未达门槛游戏数量(是否需要调整门槛?)
5. 用户争议数量(是否有大量投诉?)

**复盘时间点**: 第一届结束后1周内

---

## 七、第二届迭代建议

### 7.1 基于第一届数据的决策

**第一届结束后需要回答的问题**:
1. ✅ 评分分布是否正常?(是否正态?是否有长尾?)
2. ✅ 是否发现明显的互评团行为?
3. ✅ 是否有"好游戏被埋没"的冷启动问题?
4. ✅ 用户对排名规则的接受度如何?
5. ✅ 是否有大量"不公平"的投诉?

### 7.2 第二届可选升级

**根据第一届情况选择性升级**:

| 第一届发现的问题 | 第二届升级方案 | 复杂度 |
|----------------|--------------|--------|
| 极端值问题严重 | Winsorized → Huber M-估计量 | 中 |
| 冷启动问题严重 | 固定k → 动态k贝叶斯 | 中 |
| 互评团问题 | 事后分析 → 实时谱聚类检测 | 高 |
| 普遍接受 | 考虑简化版RTS(2维度) | 中 |

**重要**: 如果第一届运行良好,**无需强行升级**

---

## 八、最终回执结论

### 8.1 核心建议

#### ✅ 第一届采纳

1. **Winsorized Mean(截尾均值)**
   - 去掉上下各10%极端评分
   - 简单、透明、可解释
   - 无需迭代,必收敛

2. **固定k=20的贝叶斯平滑**
   - 解决冷启动问题
   - 小样本向全局均值回归
   - 大样本几乎不受影响

3. **简单的规则过滤**
   - 自评排除
   - 高危监测+人工复核
   - 不搞复杂的评分者权重

#### ❌ 第一届暂缓

1. **Huber M-估计量** - 收敛性风险,可解释性差
2. **动态置信度** - 除零风险,无历史数据支撑
3. **评分者信誉权重** - 公平性争议,实施成本高
4. **互评团实时检测** - 性能开销大,第一届规模小

#### 🎯 第二届评估

- 基于第一届数据决定是否升级
- 如果第一届运行良好,可能无需大幅改动
- 如果第一届发现明显问题,针对性解决

### 8.2 回执签名

**研判人**: 老黑(Claude)
**回执日期**: 2025-01-06
**文档版本**: v1.0
**状态**: 待蜡烛先生确认

---

## 附录: 技术术语对照表

| 技术文档术语 | 惊蛰计划采纳情况 | 替代/简化方案 |
|------------|----------------|------------|
| RRW公式 | ❌ 不采纳 | 第一届用简单规则过滤 |
| Huber Mean | ❌ 不采纳 | Winsorized Mean(截尾均值) |
| 动态贝叶斯 | ⚠️ 部分采纳 | 固定k=20贝叶斯平滑 |
| RTS系统 | ❌ 不采纳 | 第二届考虑简化版 |
| 谱聚类检测 | ❌ 不采纳 | 事后离线分析 |
| 不确定性惩罚 | ❌ 不采纳 | Tie-breaker规则 |
| 自助法区间 | ❌ 不采纳 | 第二届评估 |
| 层次贝叶斯 | ❌ 不采纳 | 过于复杂,MVP不需要 |

---

**下一步行动**: 请蜡烛先生确认本回执报告,确认后我将继续推进技术方案详细设计。
